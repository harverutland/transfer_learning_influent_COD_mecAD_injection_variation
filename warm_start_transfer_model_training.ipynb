{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea655840",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'libraries_imported' not in globals():\n",
    "    libraries_imported = False\n",
    "\n",
    "if not libraries_imported:\n",
    "    # --- Core Libraries ---\n",
    "    import os\n",
    "    import sys\n",
    "    import json\n",
    "    import random\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    # --- Data Manipulation ---\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # --- Visualization ---\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    # --- Scientific Computing ---\n",
    "    from scipy.interpolate import make_interp_spline\n",
    "    from scipy.signal import butter, filtfilt\n",
    "\n",
    "    # --- Machine Learning (scikit-learn) ---\n",
    "    from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "    from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "    from sklearn.metrics import (\n",
    "        mean_squared_error, r2_score, mean_absolute_error,\n",
    "        accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "        precision_score, recall_score, f1_score\n",
    "    )\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "    # --- Imbalanced Data ---\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "\n",
    "    # --- Deep Learning (TensorFlow / Keras) ---\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import Sequential\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import (\n",
    "        Input, Dense, LSTM, Dropout, BatchNormalization\n",
    "    )\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "    # --- Hyperparameter Tuning ---\n",
    "    from keras_tuner import RandomSearch, HyperModel\n",
    "\n",
    "    # --- Gradient Boosting ---\n",
    "    import xgboost as xgb\n",
    "\n",
    "    # --- Utilities ---\n",
    "    import joblib\n",
    "\n",
    "    # Mark libraries as imported\n",
    "    libraries_imported = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "674f9b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline_for_dataset(df_raw, dataset_tag):\n",
    "    \"\"\"\n",
    "    Trains XGB on binned TARGET & saves only final model + scaler.\n",
    "    \"\"\"\n",
    "    outdir = os.path.join(BASE_OUTDIR, dataset_tag)\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    df = df_raw.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df.sort_values('date', inplace=True)\n",
    "\n",
    "    # --- NEW: keep only 2nd half (time-ordered) ---\n",
    "    df = df.iloc[len(df)//2:].copy()\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    df = df.interpolate(method='ffill').ffill()\n",
    "\n",
    "    if TARGET not in df.columns:\n",
    "        raise ValueError(f\"[{dataset_tag}] Target column '{TARGET}' not found.\")\n",
    "    df.dropna(subset=[TARGET], inplace=True)\n",
    "\n",
    "    # bin target\n",
    "    eq_cod_min = df[TARGET].min()\n",
    "    eq_cod_max = df[TARGET].max()\n",
    "    bins = np.linspace(eq_cod_min, eq_cod_max, N_BINS + 1)\n",
    "    bins = [round_to_nearest(b, BIN_BASE) for b in bins]\n",
    "    if len(set(bins)) < len(bins):  # degenerate case\n",
    "        base = round_to_nearest(eq_cod_min, BIN_BASE)\n",
    "        bins = [base + i * BIN_BASE for i in range(N_BINS + 1)]\n",
    "    labels = [f\"{low}-{high}\" for low, high in zip(bins[:-1], bins[1:])]\n",
    "    df['fostac_category'] = pd.cut(df[TARGET], bins=bins, labels=labels, include_lowest=True)\n",
    "    df.dropna(subset=['fostac_category'], inplace=True)\n",
    "\n",
    "    # features & labels  (EXCLUDE eq_cod from inputs)\n",
    "    X = df.select_dtypes(include=[np.number]).drop(columns=[TARGET], errors=\"ignore\")\n",
    "    y = df['fostac_category'].cat.codes\n",
    "\n",
    "    # split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    # pick hyperparams via CV, but don't save/print metrics\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    best_model = None\n",
    "    best_acc = -np.inf\n",
    "    best_params = None\n",
    "    for tr_idx, val_idx in skf.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[tr_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        scaler = RobustScaler()\n",
    "        X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "        # safe SMOTE\n",
    "        try:\n",
    "            sm = SMOTE(random_state=RANDOM_STATE, k_neighbors=2)\n",
    "            X_tr_sm, y_tr_sm = sm.fit_resample(X_tr_scaled, y_tr)\n",
    "        except ValueError:\n",
    "            X_tr_sm, y_tr_sm = X_tr_scaled, y_tr\n",
    "\n",
    "        for _ in range(N_RANDOM_PARAM_ITER):\n",
    "            params = random_param_sample(PARAM_GRID)\n",
    "            model = xgb.XGBClassifier(\n",
    "                use_label_encoder=False,\n",
    "                eval_metric='mlogloss',\n",
    "                objective='multi:softmax',\n",
    "                num_class=len(labels),\n",
    "                random_state=RANDOM_STATE,\n",
    "                **params\n",
    "            )\n",
    "            model.fit(X_tr_sm, y_tr_sm)\n",
    "            acc = accuracy_score(y_val, model.predict(X_val_scaled))\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_model = (model, scaler)\n",
    "                best_params = params  # kept in-memory only\n",
    "\n",
    "    # retrain with best params on full train (still not saving metrics)\n",
    "    final_scaler = RobustScaler()\n",
    "    X_train_scaled_full = final_scaler.fit_transform(X_train)\n",
    "    X_test_scaled = final_scaler.transform(X_test)\n",
    "\n",
    "    try:\n",
    "        sm = SMOTE(random_state=RANDOM_STATE, k_neighbors=2)\n",
    "        X_train_full_sm, y_train_full_sm = sm.fit_resample(X_train_scaled_full, y_train)\n",
    "    except ValueError:\n",
    "        X_train_full_sm, y_train_full_sm = X_train_scaled_full, y_train\n",
    "\n",
    "    final_model = xgb.XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss',\n",
    "        objective='multi:softmax',\n",
    "        num_class=len(labels),\n",
    "        random_state=RANDOM_STATE,\n",
    "        **(best_params or random_param_sample(PARAM_GRID))\n",
    "    )\n",
    "    final_model.fit(X_train_full_sm, y_train_full_sm)\n",
    "    print(final_scaler.feature_names_in_)\n",
    "\n",
    "    # Save ONLY model & scaler (as requested)\n",
    "    joblib.dump(final_model, os.path.join(outdir, f\"{dataset_tag}_best_model.pkl\"))\n",
    "    joblib.dump(final_scaler, os.path.join(outdir, f\"{dataset_tag}_scaler.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a4063a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "Direction: D1 → D2\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "# ===== Warm-start both ways: D1→D2 and D2→D1 =====\n",
    "# Updated to report performance in the SAME STYLE as the baseline script:\n",
    "#  - prints Best CV accuracy + Best hyperparameters\n",
    "#  - fits final model\n",
    "#  - evaluates once on held-out validation split\n",
    "#  - plots confusion matrix\n",
    "\n",
    "import os, gc, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "\n",
    "# ------------------ Config ------------------\n",
    "OUTDIR_BASE = \"transfer_learning_model_params/warm_start\"\n",
    "os.makedirs(OUTDIR_BASE, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TARGET = \"eq_cod\"\n",
    "N_BINS = 4\n",
    "BIN_BASE = 500\n",
    "SPLIT_RATIO = 0.30\n",
    "CLASS_LABELS = [\"LL\", \"ML\", \"MH\", \"HH\"]\n",
    "\n",
    "# ---- tuning budget ----\n",
    "N_ITER = 50\n",
    "\n",
    "# ------------------ Helpers ------------------\n",
    "def round_to_nearest(x, base=500):\n",
    "    return int(base * round(float(x) / base))\n",
    "\n",
    "def random_param_sample(grid):\n",
    "    return {k: random.choice(v) for k, v in grid.items()}\n",
    "\n",
    "def build_design_matrix(df, feature_names):\n",
    "    \"\"\"Exact order for scaler/model; fill missing features with 0.0.\"\"\"\n",
    "    cols = []\n",
    "    for name in feature_names:\n",
    "        if name in df.columns:\n",
    "            cols.append(df[name])\n",
    "        else:\n",
    "            cols.append(pd.Series(0.0, index=df.index, name=name))\n",
    "    return pd.concat(cols, axis=1)\n",
    "\n",
    "def bin_and_encode(df, target_col, n_bins=4, base=500, class_labels=None):\n",
    "    \"\"\"Create bins on target and return encoded labels (0..3) aligned to class_labels order.\"\"\"\n",
    "    if class_labels is None:\n",
    "        class_labels = [\"LL\", \"ML\", \"MH\", \"HH\"]\n",
    "    vmin, vmax = df[target_col].min(), df[target_col].max()\n",
    "    bins = np.linspace(vmin, vmax, n_bins + 1)\n",
    "    bins = [round_to_nearest(b, base) for b in bins]\n",
    "    bins[0]  = min(bins[0],  vmin)\n",
    "    bins[-1] = max(bins[-1], vmax)\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"target_label\"] = pd.cut(\n",
    "        df[target_col], bins=bins, labels=class_labels, include_lowest=True\n",
    "    )\n",
    "    df = df.dropna(subset=[\"target_label\"])\n",
    "    df[\"target_label\"] = (\n",
    "        df[\"target_label\"]\n",
    "        .astype(\"category\")\n",
    "        .cat.set_categories(class_labels, ordered=True)\n",
    "    )\n",
    "    y_codes = df[\"target_label\"].cat.codes\n",
    "    return df, y_codes, bins\n",
    "\n",
    "def prepare_df_for_transfer(raw_df, dayfirst=True):\n",
    "    \"\"\"Sort, ffill, drop only rows missing the target.\"\"\"\n",
    "    df = raw_df.copy()\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\", dayfirst=dayfirst)\n",
    "        df.sort_values(\"date\", inplace=True)\n",
    "    df = df.interpolate(method=\"ffill\").ffill()\n",
    "    df = df.dropna(subset=[TARGET])\n",
    "    return df\n",
    "\n",
    "def smote_safe(X, y, random_state=42):\n",
    "    \"\"\"SMOTE with safety around tiny classes; returns possibly-resampled X,y.\"\"\"\n",
    "    y = pd.Series(y)\n",
    "    counts = Counter(y)\n",
    "    if not counts:\n",
    "        return X, y.values\n",
    "    maj = max(counts.values())\n",
    "    minority_counts = [c for c in counts.values() if c > 1]\n",
    "    min_count = min(minority_counts) if minority_counts else 0\n",
    "    k_neighbors = max(1, min(3, min_count - 1)) if min_count > 0 else 1\n",
    "    sampling_strategy = {cls: maj for cls, cnt in counts.items() if cnt > 1}\n",
    "    if sampling_strategy and min_count > 0:\n",
    "        sm = SMOTE(\n",
    "            random_state=random_state,\n",
    "            k_neighbors=k_neighbors,\n",
    "            sampling_strategy=sampling_strategy\n",
    "        )\n",
    "        return sm.fit_resample(X, y)\n",
    "    return X, y.values\n",
    "\n",
    "def warm_start_direction(src_tag, src_df, src_model_path, src_scaler_path,\n",
    "                         tgt_tag, tgt_df, outdir_base=OUTDIR_BASE):\n",
    "    print(f\"\\n====================\")\n",
    "    print(f\"Direction: {src_tag} → {tgt_tag}\")\n",
    "    print(f\"====================\")\n",
    "\n",
    "    outdir = os.path.join(outdir_base, f\"{src_tag}_to_{tgt_tag}\")\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    finetuned_model_path = os.path.join(outdir, f\"{tgt_tag}_warmstart_from_{src_tag}.pkl\")\n",
    "    sensor_csv_path      = os.path.join(outdir, f\"{tgt_tag}_sensor_predictions_from_{src_tag}.csv\")\n",
    "\n",
    "    # 1) Prepare target df\n",
    "    df_tgt = prepare_df_for_transfer(tgt_df, dayfirst=True)\n",
    "\n",
    "    # 2) Bin & encode\n",
    "    df_tgt, y_all, _ = bin_and_encode(df_tgt, TARGET, n_bins=N_BINS, base=BIN_BASE, class_labels=CLASS_LABELS)\n",
    "\n",
    "    # 3) Load src scaler/model\n",
    "    scaler = joblib.load(src_scaler_path)\n",
    "    base_model = joblib.load(src_model_path)\n",
    "\n",
    "    feature_order = list(getattr(scaler, \"feature_names_in_\", []))\n",
    "    if not feature_order:\n",
    "        raise ValueError(f\"Scaler at {src_scaler_path} missing feature_names_in_.\")\n",
    "\n",
    "    X_all = build_design_matrix(df_tgt, feature_order)\n",
    "\n",
    "    # ---- RANDOM SPLIT ----\n",
    "    rng = np.random.default_rng(RANDOM_STATE)\n",
    "    idx = rng.permutation(len(df_tgt))\n",
    "    train_size = int(len(df_tgt) * SPLIT_RATIO)\n",
    "\n",
    "    train_idx = idx[:train_size]\n",
    "    val_idx   = idx[train_size:]\n",
    "\n",
    "    X_train = X_all.iloc[train_idx].copy()\n",
    "    X_val   = X_all.iloc[val_idx].copy()\n",
    "    y_train = y_all.iloc[train_idx].copy()\n",
    "    y_val   = y_all.iloc[val_idx].copy()\n",
    "\n",
    "    val_dates = df_tgt.iloc[val_idx][\"date\"]\n",
    "\n",
    "    # ---- Neutralize eq_cod (training mean) ----\n",
    "    if \"eq_cod\" in X_train.columns:\n",
    "        neutral = X_train[\"eq_cod\"].mean()\n",
    "        X_train.loc[:, \"eq_cod\"] = neutral\n",
    "        X_val.loc[:, \"eq_cod\"] = neutral\n",
    "\n",
    "    # Scale (use src scaler)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Hyperparam search: 3-fold CV on TRAIN ONLY (same style as baseline)\n",
    "    # -----------------------------\n",
    "    param_grid = {\n",
    "        \"max_depth\": [3, 4, 5, 6, 7, 8],\n",
    "        \"learning_rate\": [0.005, 0.01, 0.05, 0.1, 0.2],\n",
    "        \"n_estimators\": [100, 200, 400, 800],\n",
    "        \"subsample\": [0.6, 0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "        \"gamma\": [0.0, 0.1, 0.2, 0.5],\n",
    "        \"min_child_weight\": [1, 3, 5, 10],\n",
    "        \"reg_lambda\": [0.5, 1.0, 2.0, 5.0],\n",
    "    }\n",
    "\n",
    "    NUM_CLASS = len(CLASS_LABELS)\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    best_params = None\n",
    "    best_cv_acc = -np.inf\n",
    "\n",
    "    for i in range(N_ITER):\n",
    "        params = random_param_sample(param_grid)\n",
    "        fold_accs = []\n",
    "\n",
    "        for tr_idx, va_idx in skf.split(X_train_scaled, y_train):\n",
    "            X_tr = X_train_scaled[tr_idx]\n",
    "            y_tr = y_train.iloc[tr_idx]\n",
    "            X_va = X_train_scaled[va_idx]\n",
    "            y_va = y_train.iloc[va_idx]\n",
    "\n",
    "            # SMOTE on fold-train only\n",
    "            X_tr_sm, y_tr_sm = smote_safe(X_tr, y_tr, random_state=RANDOM_STATE)\n",
    "\n",
    "            model = xgb.XGBClassifier(\n",
    "                use_label_encoder=False,\n",
    "                eval_metric=\"mlogloss\",\n",
    "                objective=\"multi:softmax\",\n",
    "                num_class=NUM_CLASS,\n",
    "                random_state=RANDOM_STATE,\n",
    "                nthread=1,\n",
    "                **params\n",
    "            )\n",
    "            model.fit(X_tr_sm, y_tr_sm, xgb_model=base_model.get_booster())\n",
    "            fold_accs.append(accuracy_score(y_va, model.predict(X_va)))\n",
    "\n",
    "        mean_acc = float(np.mean(fold_accs)) if fold_accs else -np.inf\n",
    "        if mean_acc > best_cv_acc:\n",
    "            best_cv_acc = mean_acc\n",
    "            best_params = params\n",
    "\n",
    "    print(f\"Best CV accuracy: {best_cv_acc:.4f}\")\n",
    "    print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Final train on full TRAIN, evaluate once on held-out VAL (same style as baseline)\n",
    "    # -----------------------------\n",
    "    X_train_sm, y_train_sm = smote_safe(X_train_scaled, y_train, random_state=RANDOM_STATE)\n",
    "\n",
    "    final_model = xgb.XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=NUM_CLASS,\n",
    "        random_state=RANDOM_STATE,\n",
    "        nthread=1,\n",
    "        **(best_params or {})\n",
    "    )\n",
    "    final_model.fit(X_train_sm, y_train_sm, xgb_model=base_model.get_booster())\n",
    "\n",
    "    y_pred = final_model.predict(X_val_scaled)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_val, y_pred)\n",
    "    mf1 = f1_score(y_val, y_pred, average=\"macro\")\n",
    "    cm = confusion_matrix(y_val, y_pred, labels=[0, 1, 2, 3])\n",
    "\n",
    "    print(f\"Hold-out accuracy:         {acc:.4f}\")\n",
    "    print(f\"Hold-out balanced accuracy:{bacc:.4f}\")\n",
    "    print(f\"Hold-out macro-F1:         {mf1:.4f}\")\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASS_LABELS)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save predictions + model (same outputs as before)\n",
    "    # -----------------------------\n",
    "    sensor_pred_codes = y_pred\n",
    "    sensor_pred_labels = pd.Categorical(\n",
    "        sensor_pred_codes, categories=[0, 1, 2, 3]\n",
    "    ).rename_categories(CLASS_LABELS).astype(str)\n",
    "\n",
    "    sensor_df = pd.DataFrame({\n",
    "        \"date\": val_dates,\n",
    "        \"pred_bin_code\": sensor_pred_codes,\n",
    "        \"pred_bin_label\": sensor_pred_labels\n",
    "    }).set_index(\"date\")\n",
    "\n",
    "    sensor_df.to_csv(sensor_csv_path)\n",
    "    joblib.dump(final_model, finetuned_model_path)\n",
    "\n",
    "    print(f\"Saved model → {finetuned_model_path}\")\n",
    "    print(f\"Saved predictions → {sensor_csv_path}\")\n",
    "\n",
    "    del base_model, final_model\n",
    "    gc.collect()\n",
    "\n",
    "# ------------------ Run both directions ------------------\n",
    "D1_TAG = \"D1\"\n",
    "D2_TAG = \"D2\"\n",
    "\n",
    "D1_MODEL = \"transfer_learning_model_params/D1/D1_best_model.pkl\"\n",
    "D1_SCALER= \"transfer_learning_model_params/D1/D1_scaler.pkl\"\n",
    "\n",
    "D2_MODEL = \"transfer_learning_model_params/D2/D2_best_model.pkl\"\n",
    "D2_SCALER= \"transfer_learning_model_params/D2/D2_scaler.pkl\"\n",
    "\n",
    "warm_start_direction(D1_TAG, dataset1_df, D1_MODEL, D1_SCALER, D2_TAG, dataset2_df)\n",
    "warm_start_direction(D2_TAG, dataset2_df, D2_MODEL, D2_SCALER, D1_TAG, dataset1_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
